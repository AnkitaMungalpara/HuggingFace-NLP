{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a97ba5f56e91489bb08e4ed8e65567e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c6322077d984aaf9dd77c1b9883ec04",
              "IPY_MODEL_a8fa0814f1f8476986ec6d0b4b0665da",
              "IPY_MODEL_17fd8086ee13442a9131c2c89c193d1b"
            ],
            "layout": "IPY_MODEL_f63502b2199342bc969f15b7d41ee906"
          }
        },
        "4c6322077d984aaf9dd77c1b9883ec04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70f2ce97f4e24bec9f28a358ddbd8ba8",
            "placeholder": "​",
            "style": "IPY_MODEL_94b6d2ba86104bd8a23887d695dd422a",
            "value": "Map: 100%"
          }
        },
        "a8fa0814f1f8476986ec6d0b4b0665da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e1979bd258e4e1bb5d933c090f5e83d",
            "max": 408,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee11b142edda43649f90b58cb04623c2",
            "value": 408
          }
        },
        "17fd8086ee13442a9131c2c89c193d1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05f350ce105b4ff4a6c0b2c2cc77571d",
            "placeholder": "​",
            "style": "IPY_MODEL_b333f63dc17946a7b21feca4a4f70252",
            "value": " 408/408 [00:00&lt;00:00, 1426.74 examples/s]"
          }
        },
        "f63502b2199342bc969f15b7d41ee906": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70f2ce97f4e24bec9f28a358ddbd8ba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94b6d2ba86104bd8a23887d695dd422a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e1979bd258e4e1bb5d933c090f5e83d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee11b142edda43649f90b58cb04623c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "05f350ce105b4ff4a6c0b2c2cc77571d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b333f63dc17946a7b21feca4a4f70252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "eOWSxcHWkRWy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how we can train a sequence classifier on one batch in PyTorch:"
      ],
      "metadata": {
        "id": "Fk9V00Q-gpfk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eh7aXi7FgDzf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a model and tokenizer instance from pre-traiend weights\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZHSf1S7gMoS",
        "outputId": "47756f46-3b93-4fbd-bec8-c73810330d66"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define sequences\n",
        "sequences = [\n",
        "              \"I am learning new natural language processing (NLP) techniques.\",\n",
        "              \"This course is amazing!\"\n",
        "          ]"
      ],
      "metadata": {
        "id": "9P2d4i5uhXAH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create batch of these sequences\n",
        "batch = tokenizer(\n",
        "                    sequences,\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    return_tensors ='pt'\n",
        "              )"
      ],
      "metadata": {
        "id": "wZAyXr0oiTZM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWN2Z7hVipKR",
        "outputId": "23393b65-2b36-4641-a7f2-e189a553812e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  1045,  2572,  4083,  2047,  3019,  2653,  6364,  1006, 17953,\n",
              "          2361,  1007,  5461,  1012,   102],\n",
              "        [  101,  2023,  2607,  2003,  6429,   999,   102,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define labels for each sequence."
      ],
      "metadata": {
        "id": "YXabOOeui8sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch['labels'] = torch.tensor([1, 1])"
      ],
      "metadata": {
        "id": "vMP37PV8it_0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZWKEOUrjHS-",
        "outputId": "5325be1b-1e41-47eb-823f-4dc8e7000a21"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  1045,  2572,  4083,  2047,  3019,  2653,  6364,  1006, 17953,\n",
              "          2361,  1007,  5461,  1012,   102],\n",
              "        [  101,  2023,  2607,  2003,  6429,   999,   102,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([1, 1])}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time to define the optimizer. Here, we use `AdamW` optimizer. It is a varient of the `Adam` optimizer. You can find more details\n",
        "[here](https://huggingface.co/docs/bitsandbytes/main/en/reference/optim/adamw)."
      ],
      "metadata": {
        "id": "9vyxz_mNjRre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define optimizer\n",
        "optimizer = AdamW(model.parameters())"
      ],
      "metadata": {
        "id": "KfK9MxFnjH8B"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the loss\n",
        "loss = model(**batch).loss"
      ],
      "metadata": {
        "id": "4Jk8lEJ9kN7E"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "vJ8ov1rokd8m"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.step()"
      ],
      "metadata": {
        "id": "HGHV0k3Qkl76"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We have just seen how we can train a model, calculate the loss, and do the backward propagation. But it has just seizures, and we know our model will not give good results on that.\n",
        "\n",
        "Now, we need to define a larger dataset and train a model on that.\n",
        "\n",
        "## Loading dataset from Hub\n",
        "\n",
        "So for that purpose we are using the MRPC (Microsoft Research Paraphrse Corpus) dataset.\n",
        "\n",
        "* It contains 5,801 pairs of sentences.\n",
        "* and a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing).\n",
        "\n",
        "This dataset was introduced in a [paper](https://aclanthology.org/I05-5002.pdf) called **Automatically Constructing a Corpus of Sentential Paraphrases** by William Dolan and Chris Brockett."
      ],
      "metadata": {
        "id": "OBct6jVok4bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "JGTR2YHdkntS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset = load_dataset('glue', 'mrpc')\n",
        "raw_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmyZYTZDoCT4",
        "outputId": "997d3f4a-bf34-4f48-9e93-b7a8ee92ff23"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 3668\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 408\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 1725\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We can access the sentences in our `raw_dataset` with the help of indexing, like with a dictionary.\n"
      ],
      "metadata": {
        "id": "_2b0YnT_pk8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_dataset = raw_dataset[\"train\"]\n",
        "raw_train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2EqoiQZoeY3",
        "outputId": "b589a28a-7e54-4f98-dcbe-d072efa9d1ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
              " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
              " 'label': 1,\n",
              " 'idx': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore the `features` of our training dataset to see which integer corresponds to which label."
      ],
      "metadata": {
        "id": "2z2QekXoqAw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_dataset.features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iePoBjvAp1Ak",
        "outputId": "b4818466-131a-450f-d1db-5ae0b1246684"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence1': Value(dtype='string', id=None),\n",
              " 'sentence2': Value(dtype='string', id=None),\n",
              " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
              " 'idx': Value(dtype='int32', id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, `label` is of type `ClassLabel`, and `0` corresponds to `not_equivalent`, and `1` corresponds to `equivalent` here."
      ],
      "metadata": {
        "id": "xa5_WNrGROi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing of a Dataset"
      ],
      "metadata": {
        "id": "DfZDOW9IRgYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we tokenize the sentences with the help of `Auotokenizer` to convert text into numbers."
      ],
      "metadata": {
        "id": "3oGBCAkwSVxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "saxkUePuqNTq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "SFtXlME2SLmw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sent1 = tokenizer(raw_dataset['train']['sentence1'])\n",
        "tokenized_sent2 = tokenizer(raw_dataset['train']['sentence2'])"
      ],
      "metadata": {
        "id": "wv5PFkWHSU4B"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can't just pass two sequences to the model and get a prediction of whether two sentences are paragraphs or not.\n",
        "\n",
        "Let's see how we can handle two sequences with the below example:"
      ],
      "metadata": {
        "id": "sKmI7MgvS2RH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"This is first sentence\", \"This is second sentence.\")\n",
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ed3znQuESjPA",
        "outputId": "80242770-0d4a-48c8-f57a-783f2d06e3de"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2023, 2003, 2034, 6251, 102, 2023, 2003, 2117, 6251, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, if you have noticed, `token_type_ids` actually represents which part is the first sentence and which part is the second sentence.\n",
        "\n",
        "Now, let's decode it back to words:"
      ],
      "metadata": {
        "id": "GTSZKqGgTY4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNM_u8F9TXol",
        "outputId": "a4600e3f-e100-430b-dc11-c72ffd6f18e0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'this',\n",
              " 'is',\n",
              " 'first',\n",
              " 'sentence',\n",
              " '[SEP]',\n",
              " 'this',\n",
              " 'is',\n",
              " 'second',\n",
              " 'sentence',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above output, we can tell that model expects sequences to be in the form of `[CLS] sentence1 [SEP] sentence2 [SEP]` for two sentences.\n",
        "\n",
        "Now that we have observed how our tokenizer can deal with pairs of sentences, we can utilize this method to tokenize our dataset."
      ],
      "metadata": {
        "id": "EGVp_SRAUAdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenizer(\n",
        "                          raw_dataset[\"train\"][\"sentence1\"],\n",
        "                          raw_dataset[\"train\"][\"sentence2\"],\n",
        "                          padding=True,\n",
        "                          truncation=True\n",
        "                      )"
      ],
      "metadata": {
        "id": "-WMkIV5tT39-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenized_function(example):\n",
        "  return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
      ],
      "metadata": {
        "id": "bnzz_jDfVbWx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that here we are not adding a `padding` argument to the function as it is not efficient. We will add it later when we create a bath  and apply padding in the respective batch. So, we need to pad to the maximum length in the batch, not to the maximum length in the entire dataset. In doing so, we can save a lot of time."
      ],
      "metadata": {
        "id": "QbEz1QxJPLWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = raw_dataset.map(tokenized_function, batched=True)\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344,
          "referenced_widgets": [
            "a97ba5f56e91489bb08e4ed8e65567e6",
            "4c6322077d984aaf9dd77c1b9883ec04",
            "a8fa0814f1f8476986ec6d0b4b0665da",
            "17fd8086ee13442a9131c2c89c193d1b",
            "f63502b2199342bc969f15b7d41ee906",
            "70f2ce97f4e24bec9f28a358ddbd8ba8",
            "94b6d2ba86104bd8a23887d695dd422a",
            "0e1979bd258e4e1bb5d933c090f5e83d",
            "ee11b142edda43649f90b58cb04623c2",
            "05f350ce105b4ff4a6c0b2c2cc77571d",
            "b333f63dc17946a7b21feca4a4f70252"
          ]
        },
        "id": "ZoIXazzDV_K2",
        "outputId": "5cb7e9ab-bc45-4a1a-fd0a-e3f220c7c1f7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a97ba5f56e91489bb08e4ed8e65567e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 3668\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 408\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 1725\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamic Padding"
      ],
      "metadata": {
        "id": "HpG-_tdzQurb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When preparing batches of data for training, we need a function to put the samples together, called a `collate function`. By default, this function converts samples to PyTorch tensors and combines them. However, if your input data varies in size, this default approach won't work well.\n",
        "\n",
        "To handle varying sizes, you can delay adding padding until you create each batch. This minimizes unnecessary padding, speeding up training. But keep in mind, if you're using a TPU, this might cause issues since TPUs prefer consistent input shapes, even if it means adding more padding.\n",
        "\n",
        "The 🤗 Transformers library provides a tool called `DataCollatorWithPadding` to help with this. It automatically applies the right amount of padding for each batch based on the tokenizer you use, ensuring the inputs are correctly padded where needed."
      ],
      "metadata": {
        "id": "BC7WLtB3VV7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding"
      ],
      "metadata": {
        "id": "G0JAmzGgP3cD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "nRsFGZXtSJPT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare some samples from our training set for batching, we first remove unnecessary columns (like `idx`, `sentence1`, and `sentence2`) because they contain strings that can't be converted into tensors. After that, we check the lengths of each entry in the batch to ensure they can be processed together."
      ],
      "metadata": {
        "id": "A6n22V1jVFuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples = tokenized_dataset[\"train\"][:6]\n",
        "samples = {k:v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}"
      ],
      "metadata": {
        "id": "mUmKkfT4SZoP"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([len(x) for x in samples[\"input_ids\"]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx6r3lFuS3ic",
        "outputId": "4cc5b795-ef16-421d-bc29-5b6ce0f8dbc0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50, 59, 47, 67, 59, 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamic padding adjusts the length of samples in a batch so that all samples are the same length, based on the longest one in that batch. In above example, the samples vary in length from 47 to 67. With dynamic padding, each sample in the batch is padded to a length of 67, the longest sample in that batch. This avoids padding all samples in the entire dataset to the length of the longest possible sample, which would waste resources."
      ],
      "metadata": {
        "id": "1jxKNId0UqEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = data_collator(samples)"
      ],
      "metadata": {
        "id": "54bvFF1bTAVa"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print({k:v.shape for k, v in batch.items()})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LfTUbyeTdPZ",
        "outputId": "bea53efa-5df3-4e77-e562-341309abeac9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': torch.Size([6, 67]), 'token_type_ids': torch.Size([6, 67]), 'attention_mask': torch.Size([6, 67]), 'labels': torch.Size([6])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well done! In this notebook we converted raw text to batches our model can deal with. We are ready to fine-tune it."
      ],
      "metadata": {
        "id": "qw3Q0v4oWu78"
      }
    }
  ]
}